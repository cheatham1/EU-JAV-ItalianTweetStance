{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Upload a saved model: finetuned BERT model for classification",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cheatham1/EU-JAV-ItalianTweetStance/blob/main/Upload_a_saved_model_finetuned_BERT_model_for_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FfqM7xWqzBK"
      },
      "source": [
        "# Upload a finetuned model and run\n",
        "\n",
        "## EU-JAV: Understanding the vaccine stance of Italian tweets\n",
        "\n",
        "We have finetuned a Transformer-based machine learning model for analysing the vaccine stance of Italian tweets. \n",
        "\n",
        "Two datasets were collected and the tweets labelled for stance.\n",
        "* dataset A: tweets between November 2019 and June 2020\n",
        "* dataset B: tweets from April to September 2021.\n",
        "\n",
        "XLM-RoBERTa-large model was finetuned using dataset A and dataset B training sets. \n",
        "\n",
        "Here we show how to load the finetuned model and test data, then run the model and plot the resulting stance classifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "LksfF7W272G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz_FIbsIqfGu"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import torch\n",
        "\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "from tqdm.auto import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model"
      ],
      "metadata": {
        "id": "jYpWrxrLWhX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"FrGes/xlm-roberta-large-finetuned-EUJAV-datasetAB\"\n",
        "\n",
        "config = AutoConfig.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, config=config)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "classifier = pipeline( task=\"text-classification\", model=model, tokenizer=tokenizer, config=config)\n"
      ],
      "metadata": {
        "id": "uLVPbzoy7gEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkHx8JHYWTU6"
      },
      "source": [
        "# Load the Dataset\n",
        "The selected dataset is loaded directly from GitHub. It is coded with labels and text in a comma-separated file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3Ik6H_eMgbi"
      },
      "source": [
        "test_data_A = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/FrGes/EU-JAV/main/datasetA_test_3categories.csv\",\n",
        "    names=[\"Annotator1\",\"Annotator2\",\"Annotator3\",\"label\", \"text\",\"index\"]\n",
        ")\n",
        "\n",
        "test_data_B = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/FrGes/EU-JAV/main/datasetB_test_3categories.csv\",\n",
        "    names=[\"Annotator1\",\"Annotator2\",\"Annotator3\",\"label\", \"text\",\"index\"]\n",
        ")\n",
        "\n",
        "test_data = test_data_A.append(test_data_B)\n",
        "\n",
        "print(\"Total test dataset: \",test_data.shape[0],\": datasetA: \", test_data_A.shape[0], \" datasetB:\", test_data_B.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "id": "C3Qk5xNeOMvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run and print Evaluation Report\n",
        "The code below runs the finetuned models on the two test datasets. \n",
        "It prints an evaluation report using tools from sklearn."
      ],
      "metadata": {
        "id": "MRoGHKrpQi9b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dngNSl50TnUL"
      },
      "source": [
        "X_test = list(test_data[\"text\"])\n",
        "y_test = list(test_data[\"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract integer values from the model prediction output\n",
        "\n",
        "y_pred = []\n",
        "\n",
        "for out in tqdm(classifier(X_test)):\n",
        "  label = out[\"label\"]\n",
        "  int_label = label.replace(\"LABEL_\", \"\")\n",
        "  y_pred.append(int(int_label))"
      ],
      "metadata": {
        "id": "W91k0AWwqO0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot results"
      ],
      "metadata": {
        "id": "jgLVXjiGhaqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "metadata": {
        "id": "ZIJAMT79ZtTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(y_pred), columns = [\"label\"] )\n",
        "counts = df['label'].value_counts()"
      ],
      "metadata": {
        "id": "GvtFfzZ6aJ9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts"
      ],
      "metadata": {
        "id": "nPrUf03InXFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot bargraph to show stance of tweets as labelled by the model\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=counts.index, y=counts.values, palette = \"Blues\")\n",
        "plt.title('Stance of Italian Tweets test datasets A and B')\n",
        "plt.ylabel('counts')\n",
        "plt.xticks([0, 1, 2], ['Promotional', 'Neutral', 'Discouraging'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0o4clws4ZkQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy and F1-score of the model calculated using the model prediction and the annotators label \n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1  = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(\"Accuracy: \", acc, \" F1-score: \",f1)"
      ],
      "metadata": {
        "id": "hF_rq1ACOM34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report showing performance of the finetuned model\n",
        "print(classification_report(y_test, y_pred, digits=3))\n"
      ],
      "metadata": {
        "id": "gye51kd4ZjBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The annotator label and model prediction shown alongside the text\n",
        "test_data.insert(loc = 4, column = \"prediction\", value = y_pred)\n",
        "test_data.head()"
      ],
      "metadata": {
        "id": "NZfMvw_xU91q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}